#!/usr/bin/python
#-*- encoding: utf8 -*-

from nugsl.nporip import npoRip
import libxml2
import os,sys
import csv
import re
from cPickle import Pickler,Unpickler
import urllib

def filt( arg ):
    r = re.match('.*?([0-9]+).*',arg)
    return r.group(1)

target = os.path.splitext( os.path.split( sys.argv[0] )[1][6:] )[0]

cwd = os.getcwd()

csvdir = cwd + '/financials/csv/'
tmpdir = cwd + '/financials/tmp/' + target + '/'
pdfdir = cwd + '/financials/pdf/' + target + '/'
statedir = cwd + '/financials/state/' + target + '/'
ppmdir = cwd + '/financials/ppm/' + target + '/'
stoplistdir = cwd + '/financials/stoplist/'

stoplist = stoplistdir + target + '.txt'

for item in [csvdir, tmpdir, pdfdir, statedir, ppmdir, stoplistdir]:
    try:
        os.makedirs( item )
    except:
        pass

# Get list of orgs already processed
state = []
        
# Get the manually maintained stoplist, if available
if os.path.exists( stoplist ):
    stoplist = open( stoplist ).read().strip()
    stoplist = stoplist.split('\n')

# Read CSV, and pop orgs with failed reads from list
killme = False
if os.path.exists( csvdir + target + '.csv'):
    ifhcr = open( csvdir + target + '.csv')
    csvreader = csv.reader( ifhcr )
    ifhcw = open( csvdir + target + '.csv.new', 'w+')
    csvwriter = csv.writer( ifhcw )
    seen = []
    for line in csvreader:
        if ' ' in line[0]:
            continue
        if int( line[1] ) == -1:
            print 'Failed OCR: %s' % line[0]
            if not line[0] in stoplist:
                killme = True
            if len( sys.argv) > 1 and sys.argv[1] == '--force':
                try:
                    state.remove( line[0] )
                except:
                    pass
                killme = False
            else:
                state.append( line[0] )
                csvwriter.writerow( line )
        else:
            state.append( line[0] )
            csvwriter.writerow( line )
    ifhcr.close()
    ifhcw.close()
    os.unlink( csvdir + target + '.csv')
    os.rename( csvdir + target + '.csv.new',csvdir + target + '.csv')

if killme:
    sys.exit()

# Open CSV file for writing
ofhc = open( csvdir + target + '.csv', 'a')
csv = csv.writer( ofhc )

### Site-specific code
    
np = npoRip()

orgs = []

if os.path.exists( statedir  + 'index.pickle'):
    ifhp = open( statedir + 'index.pickle' )
    unpickler = Unpickler( ifhp )
    orgs = unpickler.load()
    ifhp.close()
else:

    docstubs = []

    nextpage = '1'
    lastpage = '1'
    first = True
    while 1:
        PARAMS='corporation_id=&eventKey=PAGE_CHANGE&fromScreenID=&page=%s'
        if first:
            params = 'corporation_id&'
            first = False
        else:
            params = PARAMS % nextpage
        webdoc = np.fetch(urlbase='http://npo.pref.kyoto.lg.jp',
                          stub='/npo_res/navi/E40400.do',
                          params=params,
                          encoding='utf8')
          
        res = webdoc.xpathEval('//a[contains(@onclick,"setCorporationID")]')
        ids = [(x.prop('onclick')[18:26], lastpage) for x in res]
        docstubs.extend( ids )
        
        res = webdoc.xpathEval('//a[contains(@onclick,"onPageChange")]')
        if len(res) < 2:
            print "Finished with index"
            break
        pagestring = res[-1].prop('onclick')
        r = re.match('.*?([0-9]+).*',pagestring)
        if r:
            nextpage = r.group(1)
            print nextpage
        else:
            print "Finished with index, maybe."
            break
        # Gotcha, darnit
        if int(lastpage) > int(nextpage):
            print "Finished with index"
            break
        lastpage = nextpage
    
    for docstub in docstubs:
        PARAMS2='corporation_id=%s&eventKey=LINK&fromScreenID=&page=%s'
        params = PARAMS2 % docstub
        webdoc = np.fetch(urlbase='http://npo.pref.kyoto.lg.jp',
                          stub='/npo_res/navi/E40400.do',
                          params=params,
                          encoding='utf8')
        res = webdoc.xpathEval('//th[contains(.,"名称")]/following::td')
        orgname = res[0].content
        print orgname
        res = webdoc.xpathEval('//a[contains(.,"事業報告書")]')
        if res:
            stub = res[-1].prop('href')
        else:
            continue
        orgs.append( (orgname, stub, docstub[0], docstub[1] ) )

    ofhp = open( statedir + 'index.pickle', 'w+')
    pickler = Pickler( ofhp )
    pickler.dump( orgs )
    ofhp.close()

count = 0
print len(orgs)
for org in orgs:
    orgname,reportstub, orgid, page = org
    orgname = np.sanitize( orgname )
    
    count += 1
    
    # Don't want to be bothered with shit we've put in the stoplist
    if orgname in stoplist:
        print '%d: STOP!  STOP!  %s' % (count, orgname)
        continue
    
    if orgname in state:
        print '%d: Done: %s' % (count, orgname)
        continue
    
    ### Standard code below
        
    # Clear out working directory
    for f in os.listdir(tmpdir):
        os.unlink(tmpdir + f)
        
    ### Standard code above
        
    # Fetch one PDF file
    print '%d: %s' % (count, orgname)
    sys.stdout.write('v')
    sys.stdout.flush()
    PARAMS2='corporation_id=%s&eventKey=LINK&fromScreenID=&page=%s'
    params = PARAMS2 % (org[2], org[3]) 
    webdoc = np.fetch(urlbase='http://npo.pref.kyoto.lg.jp',
                      stub='/npo_res/navi/E40400.do',
                      params=params,
                      encoding='utf8')

    headers = {'Referer':'http://npo.pref.kyoto.lg.jp/npo_res/navi/E40400.do'}

    pdf = np.fetch(urlbase='http://npo.pref.kyoto.lg.jp',
                      stub=reportstub,
                      headers=headers,
                      useget=True,
                      raw=True)

    ofhp = open(tmpdir + orgname + '.pdf','w+')
    ofhp.write( pdf )
    ofhp.close()
    sys.stdout.write('_')
    sys.stdout.flush()

    ### Standard code below
    
    # OCR magic
    expenditure,ppms = np.pdf2gocr( tmpdir + orgname + '.pdf')    
    csv.writerow( [orgname, expenditure] )

    ofh = open( ppmdir + target + '.txt', 'a')
    for ppm in ppms:
        ofh.write( ppm + '\n' )
        os.unlink( tmpdir + ppm )
    ofh.close()

    # Tidy up
    os.rename(tmpdir + orgname + '.pdf', pdfdir + orgname + '.pdf')
    
